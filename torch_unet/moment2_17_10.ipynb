{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "521751a7-3e9b-4a99-817b-7073dbbd89a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bb1e99e-d538-44a4-beb8-3c0f38c8f8c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA AND INITIALISING DATALOADERS\n",
      "INITIALISING dataloaders\n",
      "LOADING FIRST MOMENT MODEL\n",
      "INITIALISING SECOND MOMENT MODEL\n",
      "STARTING TRAINING LOOP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0001:   0%|          | 0/362 [01:25<?, ?it/s]\n",
      "current loss: -4.8602:  60%|█████▉    | 216/362 [56:16<38:02, 15.63s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 490\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 490\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss)\n\u001b[1;32m    493\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(test())\n",
      "Cell \u001b[0;32mIn [13], line 393\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    389\u001b[0m     y \u001b[38;5;241m=\u001b[39m (model_1(x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbfloat16))\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat) \u001b[38;5;241m-\u001b[39m y)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    391\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(preds, y)\n\u001b[0;32m--> 393\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mclip_grad_value_(model\u001b[38;5;241m.\u001b[39mparameters(), GRADIENT_CLIP) \u001b[38;5;66;03m# GRADIENT CLIPPING\u001b[39;00m\n\u001b[1;32m    397\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \n",
      "File \u001b[0;32m/data101/makinen/venvs/fastjax/lib/python3.9/site-packages/accelerate/accelerator.py:1985\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1984\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1985\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data101/makinen/venvs/fastjax/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data101/makinen/venvs/fastjax/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0001:   0%|          | 0/362 [1:11:21<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import helpers\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import math\n",
    "from torch.utils import data\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "import gc\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import os.path as osp\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import cloudpickle as pickle\n",
    "import sys,os,json\n",
    "\n",
    "from dataloader import *\n",
    "from nets import *\n",
    "\n",
    "from nets2_attn import *\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "        \n",
    "def load_obj(name):\n",
    "    with open(name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# custom log-mse loss\n",
    "\n",
    "\n",
    "\n",
    "class logMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        # log-transform the targets\n",
    "        #actual = transform_inputs(actual, scaling=1e5)\n",
    "\n",
    "        return torch.log(self.mse(pred, actual))\n",
    "\n",
    "\n",
    "\n",
    "def log_cosh_loss(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    def _log_cosh(x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + torch.nn.functional.softplus(-2. * x) - math.log(2.0)\n",
    "    return torch.mean(_log_cosh(y_pred - y_true))\n",
    "\n",
    "class LogCoshLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(\n",
    "        self, y_pred: torch.Tensor, y_true: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        return log_cosh_loss(y_pred, y_true)\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--config\", type=str, required=True)\n",
    "# # parser.add_argument(\"--run-name\", type=str, required=False, default=\"\")\n",
    "# # parser.add_argument(\"--nde-file\", type=str, required=False, default=\"./laptop_run_8nets/\")\n",
    "# # parser.add_argument(\"--summary-file\", type=str, required=False, default=\"final_dry_ABC_four_runs_all.pkl\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "## READ IN CONFIGS\n",
    "# config_file_path = args.config #'./comparison/configs.json'\n",
    "\n",
    "\n",
    "config_file_path =  './configs_30_09.json'\n",
    "\n",
    "\n",
    "\n",
    "with open(config_file_path) as f:\n",
    "        configs = json.load(f)\n",
    "\n",
    "\n",
    "# model stuff\n",
    "# HIDDEN_CHANNELS = configs[\"model_params\"][model_type][model_size][\"hidden_channels\"]\n",
    "# NUM_LAYERS = configs[\"model_params\"][model_type][model_size][\"num_layers\"]\n",
    "# MODEL_NAME = configs[\"model_params\"][model_type][model_size][\"name\"]\n",
    "# TEST_BATCHING = configs[\"model_params\"][model_type][model_size][\"test_batching\"]\n",
    "        \n",
    "FILTERS = configs[\"model_params\"][\"filters\"]\n",
    "NOISEAMP = configs[\"model_params\"][\"noiseamp\"]\n",
    "N_FG = configs[\"model_params\"][\"n_fg\"]\n",
    "MODEL_PATH = configs[\"model_params\"][\"model_path\"]\n",
    "MODEL_NAME = configs[\"model_params\"][\"model_name\"]\n",
    "ACTIVATION = configs[\"model_params\"][\"act\"]\n",
    "\n",
    "\n",
    "\n",
    "# # optimizer schedule\n",
    "LEARNING_RATE = configs[\"training_params\"][\"learning_rate\"]\n",
    "BATCH_SIZE = configs[\"training_params\"][\"batch_size\"]\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "EPOCHS = int(configs[\"training_params\"][\"epochs\"])\n",
    "GRADIENT_CLIP = float(configs[\"training_params\"][\"gradient_clip\"])\n",
    "SCALING = 1e5\n",
    "#DO_SCHEDULER = bool(int(configs[\"training_params\"][\"do_lr_scheduler\"]))\n",
    "SEED = int(configs[\"training_params\"][\"seed\"])\n",
    "\n",
    "# # data + out directories\n",
    "cosmopath = configs[\"training_params\"][\"cosmopath\"]\n",
    "galpath = configs[\"training_params\"][\"galpath\"]\n",
    "\n",
    "\n",
    "MODEL_DIR = configs[\"model_params\"][\"model_dir\"]\n",
    "LOAD_DIR = configs[\"model_params\"][\"load_dir\"]\n",
    "LOAD_MODEL = bool(configs[\"training_params\"][\"load_model\"])\n",
    "\n",
    "TRAIN_WITH_CACHE = False\n",
    "ADD_NOISE = configs[\"training_params\"][\"add_noise\"]\n",
    "\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "   # Create a new directory if it does not exist\n",
    "   os.makedirs(MODEL_DIR)\n",
    "   print(\"created new directory\", MODEL_DIR)\n",
    "\n",
    "# ### CONSTRUCT MODEL NAME AND OUTPUT PATH\n",
    "# MODEL_NAME += \"nc_%d_nlyr_%d\"%(HIDDEN_CHANNELS, NUM_LAYERS)\n",
    "# MODEL_PATH = MODEL_DIR + MODEL_NAME\n",
    "# LOAD_PATH = LOAD_DIR + MODEL_NAME\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "    \n",
    "print(\"LOADING DATA AND INITIALISING DATALOADERS\")\n",
    "\n",
    "# fix random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "cosmofiles = os.listdir(cosmopath)\n",
    "galfiles = os.listdir(galpath)\n",
    "\n",
    "# save the filenames \n",
    "#save_obj(cosmofiles, \"/data101/makinen/hirax_sims/dataloader/cosmofiles\")\n",
    "#save_obj(galfiles, \"/data101/makinen/hirax_sims/dataloader/galfile\")\n",
    "\n",
    "\n",
    "cosmofiles = [cosmopath + p for p in cosmofiles]\n",
    "galfiles =  [galpath + p for p in galfiles]\n",
    "\n",
    "\n",
    "# random mask for train/val split\n",
    "mask = np.random.rand(len(cosmofiles)) < 0.9\n",
    "train_cosmo_files = list(np.array(cosmofiles)[mask])[:configs[\"training_params\"][\"num_train\"]]\n",
    "val_cosmo_files = list(np.array(cosmofiles)[~mask])\n",
    "\n",
    "galmask = np.random.rand(len(galfiles)) < 0.9\n",
    "train_gal_files = list(np.array(galfiles)[galmask])\n",
    "val_gal_files = list(np.array(galfiles)[~galmask])\n",
    "\n",
    "# save the train/val masks\n",
    "#np.save(\"/data101/makinen/hirax_sims/dataloader/cosmo_mask\", mask)\n",
    "#np.save(\"/data101/makinen/hirax_sims/dataloader/gal_mask\", galmask)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(x,y):\n",
    "    \n",
    "        # split ordering (batch, baseline, freq, ra) = (batch*split, 48, 128, 128)\n",
    "        # then transpose to (batch*split, freq, ra, baseline)\n",
    "        x = torch.permute(\n",
    "            torch.cat(torch.tensor_split(x, split, dim=3)),\n",
    "            (0, 3, 1, 2)\n",
    "        )\n",
    "        y = torch.permute(\n",
    "                torch.cat(torch.tensor_split(y, split, dim=3)),\n",
    "                (0, 3, 1, 2)\n",
    "        )\n",
    "        # then finally get the real and im parts as channels\n",
    "        # shape: (batch*split, freq, ra, baseline, Re/Im)\n",
    "        x = torch.stack([x.real, x.imag], dim=-1)\n",
    "        y = torch.stack([y.real, y.imag], dim=-1)\n",
    "\n",
    "        \n",
    "        # add white noise to the signal\n",
    "        if ADD_NOISE:\n",
    "            x += torch.normal(mean=0.0, std=torch.ones(x.shape)*NOISEAMP) #.to(device)\n",
    "        \n",
    "        # pass x to the pca\n",
    "        x = PCALayer(x, N_FG=N_FG)\n",
    "\n",
    "        x *= 1e5\n",
    "        y *= 1e5\n",
    "        \n",
    "        # log-transformation of input data for network\n",
    "        #x = transform_inputs(x, scaling=1e5)\n",
    "        \n",
    "        # transformation of outputs handled in the loss function\n",
    "        #y = transform_inputs(y, scaling=1e5)\n",
    "\n",
    "        \n",
    "        # get y into same shape as model outputs\n",
    "        y = torch.permute(y, (0, 4, 2, 1, 3))\n",
    "        \n",
    "        return x.to(device),y.to(device)\n",
    "    \n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    print(\"batch\", len(batch))\n",
    "    x,y = batch\n",
    "    x,y = preprocess_data(x,y)\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# create train and val datasets and loaders with collate fn\n",
    "\n",
    "print(\"INITIALISING dataloaders\")\n",
    "train_dataset = H5Dataset(train_cosmo_files, train_gal_files, use_cache=False)\n",
    "val_dataset = H5Dataset(val_cosmo_files, val_gal_files, use_cache=False)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,  # bigger batch ?\n",
    "    num_workers=1, # how high can we go ?\n",
    "    shuffle=False,\n",
    "    pin_memory=True, # do we need this ?\n",
    "    #collate_fn=my_collate_fn\n",
    ")\n",
    "\n",
    "# the new collate function is quite generic\n",
    "#loader = DataLoader(demo, batch_size=50, shuffle=True, \n",
    "#                    collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# initialise model and accelerator\n",
    "\n",
    "\n",
    "print(\"LOADING FIRST MOMENT MODEL\")\n",
    "    \n",
    "\n",
    "split = 1024 // 128 # 8 chunks per sky simulation\n",
    "\n",
    "\n",
    "#STEPS_PER_EPOCH = 100 # reshuffle data each time \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#block = BasicBlock(16, 32)\n",
    "\n",
    "act = smooth_leaky if ACTIVATION == \"smooth_leaky\" else nn.SiLU\n",
    "model_1 = UNet3d(BasicBlock, filters=FILTERS, act=act).to(device)\n",
    "\n",
    "\n",
    "model_path = MODEL_PATH\n",
    "model_path += MODEL_NAME\n",
    "\n",
    "\n",
    "model_1.load_state_dict(torch.load(model_path + \"/pytorch_model.bin\"))\n",
    "model_1.eval()\n",
    "\n",
    "for name, para in model_1.named_parameters():\n",
    "    para.requires_grad = False\n",
    "\n",
    "\n",
    "model_1.to(torch.bfloat16)\n",
    "\n",
    "\n",
    "# FREEZE FOR NOW\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# INITIALISE MOMENT 2 --> \"MODEL\"\n",
    "print(\"INITIALISING SECOND MOMENT MODEL\")\n",
    "\n",
    "act = smooth_leaky if ACTIVATION == \"smooth_leaky\" else nn.SiLU\n",
    "model = UNet3d(BasicBlock, filters=8, act=act).to(device)\n",
    "model.to(torch.bfloat16)\n",
    "\n",
    "\n",
    "model_path = MODEL_PATH\n",
    "model_path += MODEL_NAME\n",
    "model_path += \"_moment_2\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# start up the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "def log_cosh_loss(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    def _log_cosh(x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + torch.nn.functional.softplus(-2. * x) - math.log(2.0)\n",
    "    return torch.mean(_log_cosh(y_pred - y_true))\n",
    "\n",
    "class LogCoshLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(\n",
    "        self, y_pred: torch.Tensor, y_true: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        return log_cosh_loss(y_pred, y_true)\n",
    "\n",
    "\n",
    "# criterion = LogCoshLoss()\n",
    "\n",
    "criterion = logMSELoss()\n",
    "accelerator = Accelerator(project_dir=model_path)\n",
    "\n",
    "model, optimizer, train_dataloader = accelerator.prepare(\n",
    "                model, optimizer, train_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    accelerator.load_state(model_path)\n",
    "    history = load_obj(MODEL_DIR + MODEL_NAME + \"history.pkl\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    #model.train()\n",
    "\n",
    "    pbar = tqdm(total=len(train_dataloader), position=0)\n",
    "    pbar.set_description(f'Training epoch: {epoch:04d}')\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    \n",
    "    pbar2 = tqdm(train_dataloader, leave=True, position=0)\n",
    "    for i,data in enumerate(pbar2):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        x,y = data\n",
    "        x,y = preprocess_data(x.cpu(),y.cpu())\n",
    "        \n",
    "        #y *= model.scaling # same playing field as network\n",
    "        #x *= model.scaling # scaling factor \n",
    "\n",
    "\n",
    "        # log-transformation of input data for network\n",
    "        \n",
    "        \n",
    "        # MODIFY THIS FOR SECOND MOMENT TRAINING\n",
    "        preds = model(x.to(torch.bfloat16)).to(torch.float)\n",
    "        \n",
    "        # freeze moment_1\n",
    "        with torch.no_grad():\n",
    "            y = (model_1(x.to(torch.bfloat16)).to(torch.float) - y)**2\n",
    "\n",
    "        loss = criterion(preds, y)\n",
    "        \n",
    "        accelerator.backward(loss)\n",
    "        \n",
    "        accelerator.clip_grad_value_(model.parameters(), GRADIENT_CLIP) # GRADIENT CLIPPING\n",
    "        \n",
    "        optimizer.step() \n",
    "        #if DO_SCHEDULER:\n",
    "        #    lr_scheduler.step()\n",
    "\n",
    "        total_loss += float(loss) #* int(data.train_mask.sum())\n",
    "        total_examples += 1 #data.shape #int(data.train_mask.sum())\n",
    "        \n",
    "        if not TRAIN_WITH_CACHE:\n",
    "            train_dataloader.dataset.gal_cache = []\n",
    "            train_dataloader.dataset.cosmo_cache = []\n",
    "            \n",
    "        pbar2.set_description(\"current loss: %.4f\"%(total_loss / total_examples))\n",
    "            \n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    # dump to save memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(plot=False):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "\n",
    "    #pbar = tqdm(total=len(val_dataloader), position=0)\n",
    "\n",
    "    for i,data in tqdm(enumerate(val_dataloader)):\n",
    "        #if i % 2 == 0:\n",
    "        x,y = data\n",
    "        x,y = preprocess_data(x.cpu(),y.cpu())\n",
    "                \n",
    "        preds = model(x)\n",
    "        y = (model_1(x) - y)**2\n",
    "        loss = criterion(preds, y)\n",
    "\n",
    "        preds = preds.cpu()\n",
    "        y = y.cpu()\n",
    "\n",
    "        total_loss += float(loss) #* int(data.train_mask.sum())\n",
    "        total_examples += 1 #data.shape #int(data.train_mask.sum())\n",
    "        \n",
    "        if plot:\n",
    "            plt.subplot(131)\n",
    "            plt.title(\"truth\")\n",
    "            plt.imshow(y[0, 0, 0, :, :])\n",
    "            plt.colorbar()\n",
    "\n",
    "            plt.subplot(132)\n",
    "            plt.title(\"network prediction\")\n",
    "            plt.imshow(preds[0, 0, 0, :, :])\n",
    "            plt.colorbar()\n",
    "            \n",
    "            \n",
    "            plt.subplot(133)\n",
    "            plt.title(\"residual\")\n",
    "            plt.imshow(((preds[0, 0, 0, :, :] - y[0, 0, 0, :, :])))\n",
    "            plt.colorbar()\n",
    "            \n",
    "            plt.show()\n",
    "    \n",
    "    val_dataloader.dataset.cosmo_cache = []\n",
    "    val_dataloader.dataset.gal_cache = []\n",
    "\n",
    "    return total_loss / total_examples\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# run the training loop\n",
    "    \n",
    "print(\"STARTING TRAINING LOOP\")\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# training history\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"losses\": []\n",
    "}\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "# training loop\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    loss = train(epoch)\n",
    "    loss = float(loss)\n",
    "\n",
    "    val_loss = float(test())\n",
    "\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "\n",
    "        best_loss = val_loss\n",
    "        accelerator.save_model(model, model_path)\n",
    "        accelerator.save_state(model_path)\n",
    "\n",
    "    # save history\n",
    "    history[\"train_loss\"].append(loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "    save_obj(history, MODEL_NAME + \"_train_history_inprog\") # save locally\n",
    "    save_obj(history, model_path + \"train_history_inprog\")\n",
    "\n",
    "    # dump to save memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f'Loss: {loss:.4f}')\n",
    "\n",
    "# save the history object\n",
    "save_obj(history, \"train_history\")\n",
    "save_obj(history, MODEL_NAME + \"train_history\")\n",
    "save_obj(configs, MODEL_NAME + \"configs.json\")\n",
    "\n",
    "\n",
    "# should we do the validation checks here ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aac5581-ea74-4272-ad6c-ab1533d5e6f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0001:   0%|          | 0/362 [01:56<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4216"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0eb3ba1-ba71-484e-90c2-5f091e7a9157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72e197c-a833-4c51-8bff-235125e30246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
